#!/bin/bash
# Omarchy AI Doctor - Comprehensive diagnostic and repair tool

set -euo pipefail

# Configuration
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly LOG_FILE="$HOME/.omarchy-ai-doctor.log"
readonly REPORT_FILE="$HOME/.omarchy-ai-diagnosis.txt"

# Colors for output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m' # No Color

# Status counters
CHECKS_PASSED=0
CHECKS_FAILED=0
CHECKS_WARNING=0

# Logging functions
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $*" | tee -a "$LOG_FILE"
}

error() {
    log "${RED}âŒ ERROR: $*${NC}"
    ((CHECKS_FAILED++))
}

warning() {
    log "${YELLOW}âš ï¸  WARNING: $*${NC}"
    ((CHECKS_WARNING++))
}

success() {
    log "${GREEN}âœ… SUCCESS: $*${NC}"
    ((CHECKS_PASSED++))
}

info() {
    log "${BLUE}â„¹ï¸  INFO: $*${NC}"
}

# Diagnostic functions
check_omarchy_ai_version() {
    info "Checking Omarchy AI version..."
    
    if [[ -f ~/.omarchy-ai-version ]]; then
        local version_info
        version_info=$(cat ~/.omarchy-ai-version)
        echo "$version_info"
        success "Version file found"
    else
        warning "Version file not found - installation may be incomplete"
    fi
}

check_system_requirements() {
    info "Checking system requirements..."
    
    # RAM check
    local total_ram_gb
    total_ram_gb=$(($(grep MemTotal /proc/meminfo | awk '{print $2}') / 1024 / 1024))
    
    if [[ $total_ram_gb -ge 32 ]]; then
        success "RAM: ${total_ram_gb}GB (excellent)"
    elif [[ $total_ram_gb -ge 16 ]]; then
        success "RAM: ${total_ram_gb}GB (sufficient)"
    else
        error "RAM: ${total_ram_gb}GB (insufficient - minimum 16GB required)"
    fi
    
    # Disk space check
    local available_space_gb
    available_space_gb=$(($(df / | tail -1 | awk '{print $4}') / 1024 / 1024))
    
    if [[ $available_space_gb -ge 500 ]]; then
        success "Disk space: ${available_space_gb}GB (excellent)"
    elif [[ $available_space_gb -ge 100 ]]; then
        success "Disk space: ${available_space_gb}GB (sufficient)"
    else
        error "Disk space: ${available_space_gb}GB (insufficient - minimum 100GB required)"
    fi
    
    # CPU check
    local cpu_cores
    cpu_cores=$(nproc)
    if [[ $cpu_cores -ge 8 ]]; then
        success "CPU cores: $cpu_cores (excellent)"
    elif [[ $cpu_cores -ge 4 ]]; then
        success "CPU cores: $cpu_cores (sufficient)"
    else
        warning "CPU cores: $cpu_cores (may impact performance)"
    fi
}

check_gpu_status() {
    info "Checking GPU status..."
    
    if command -v nvidia-smi >/dev/null 2>&1; then
        local gpu_info
        gpu_info=$(nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits 2>/dev/null || echo "")
        
        if [[ -n "$gpu_info" ]]; then
            echo "NVIDIA GPU detected:"
            while IFS=, read -r gpu_name gpu_memory driver_version; do
                gpu_name=$(echo "$gpu_name" | xargs)
                gpu_memory=$(echo "$gpu_memory" | xargs)
                driver_version=$(echo "$driver_version" | xargs)
                
                echo "  - $gpu_name (${gpu_memory}MB VRAM, Driver: $driver_version)"
                
                if [[ $gpu_memory -ge 24000 ]]; then
                    success "GPU VRAM: ${gpu_memory}MB (excellent - can run large models)"
                elif [[ $gpu_memory -ge 8192 ]]; then
                    success "GPU VRAM: ${gpu_memory}MB (good - can run most models)"
                elif [[ $gpu_memory -ge 4096 ]]; then
                    warning "GPU VRAM: ${gpu_memory}MB (limited - small models only)"
                else
                    warning "GPU VRAM: ${gpu_memory}MB (very limited)"
                fi
            done <<< "$gpu_info"
            
            # Test CUDA
            if command -v nvcc >/dev/null 2>&1; then
                local cuda_version
                cuda_version=$(nvcc --version | grep "release" | awk '{print $6}' | cut -c2-)
                success "CUDA toolkit: $cuda_version"
            else
                warning "CUDA toolkit not found"
            fi
        else
            warning "NVIDIA GPU detected but nvidia-smi failed"
        fi
    else
        warning "No NVIDIA GPU detected - AI inference will use CPU (slower)"
    fi
}

check_conda_environment() {
    info "Checking conda environment..."
    
    # Check if conda is available
    if command -v conda >/dev/null 2>&1; then
        success "Conda available"
        
        # Check conda version
        local conda_version
        conda_version=$(conda --version | awk '{print $2}')
        echo "Conda version: $conda_version"
        
        # Check ai-dev environment
        if conda info --envs 2>/dev/null | grep -q "ai-dev"; then
            success "ai-dev environment exists"
            
            # Test environment activation
            if conda run -n ai-dev python --version >/dev/null 2>&1; then
                local python_version
                python_version=$(conda run -n ai-dev python --version | awk '{print $2}')
                success "ai-dev environment functional (Python $python_version)"
            else
                error "ai-dev environment cannot be activated"
            fi
        else
            error "ai-dev environment not found"
        fi
    else
        error "Conda not available"
    fi
}

check_python_packages() {
    info "Checking Python packages..."
    
    if ! command -v conda >/dev/null 2>&1; then
        error "Conda not available - cannot check packages"
        return 1
    fi
    
    local critical_packages=(
        "torch"
        "transformers"
        "numpy"
        "pandas"
        "matplotlib"
        "jupyter"
        "fastapi"
        "gradio"
    )
    
    local package_status=()
    
    for package in "${critical_packages[@]}"; do
        if conda run -n ai-dev python -c "import $package; print(f'$package: OK')" 2>/dev/null; then
            success "Package $package: installed and importable"
            package_status+=("$package:OK")
        else
            error "Package $package: missing or broken"
            package_status+=("$package:FAILED")
        fi
    done
    
    # Test PyTorch CUDA support
    if conda run -n ai-dev python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')" 2>/dev/null; then
        local cuda_available
        cuda_available=$(conda run -n ai-dev python -c "import torch; print(torch.cuda.is_available())" 2>/dev/null)
        if [[ "$cuda_available" == "True" ]]; then
            success "PyTorch CUDA support: enabled"
        else
            warning "PyTorch CUDA support: disabled (CPU only)"
        fi
    else
        warning "Cannot check PyTorch CUDA support"
    fi
}

check_ai_workspace() {
    info "Checking AI workspace..."
    
    if [[ -d ~/ai-workspace ]]; then
        success "AI workspace directory exists"
        
        local required_dirs=(
            "projects"
            "models" 
            "datasets"
            "experiments"
            "notebooks"
            "logs"
            "mlruns"
        )
        
        for dir in "${required_dirs[@]}"; do
            if [[ -d ~/ai-workspace/$dir ]]; then
                success "Directory $dir: exists"
            else
                warning "Directory $dir: missing"
            fi
        done
        
        # Check .env file
        if [[ -f ~/ai-workspace/.env ]]; then
            success ".env file: exists"
        else
            warning ".env file: missing"
        fi
        
        # Check disk usage
        local workspace_size
        workspace_size=$(du -sh ~/ai-workspace 2>/dev/null | awk '{print $1}' || echo "Unknown")
        echo "AI workspace size: $workspace_size"
        
    else
        error "AI workspace directory not found"
    fi
}

check_shell_aliases() {
    info "Checking shell aliases..."
    
    local required_aliases=(
        "ai-env"
        "ai-workspace"
        "jupyter-ai"
        "model-download"
        "model-list"
        "gpu-monitor"
    )
    
    for alias_name in "${required_aliases[@]}"; do
        if alias "$alias_name" >/dev/null 2>&1; then
            success "Alias $alias_name: configured"
        else
            warning "Alias $alias_name: not found"
        fi
    done
}

check_system_services() {
    info "Checking system services..."
    
    local services=(
        "docker"
        "ai-model-server"
    )
    
    for service in "${services[@]}"; do
        if systemctl list-unit-files | grep -q "^$service.service"; then
            local status
            status=$(systemctl is-enabled "$service" 2>/dev/null || echo "not-found")
            if [[ "$status" == "enabled" ]]; then
                success "Service $service: enabled"
            else
                warning "Service $service: disabled"
            fi
        else
            info "Service $service: not installed"
        fi
    done
}

check_network_connectivity() {
    info "Checking network connectivity..."
    
    local test_urls=(
        "github.com"
        "pypi.org"
        "huggingface.co"
        "pytorch.org"
    )
    
    for url in "${test_urls[@]}"; do
        if curl -s --connect-timeout 5 "$url" >/dev/null 2>&1; then
            success "Connection to $url: OK"
        else
            warning "Connection to $url: FAILED"
        fi
    done
}

check_model_files() {
    info "Checking model files..."
    
    local model_dirs=(
        "~/ai-workspace/models/huggingface"
        "~/ai-workspace/models/gguf"
        "~/ai-workspace/models/pytorch"
    )
    
    for model_dir in "${model_dirs[@]}"; do
        local expanded_dir
        expanded_dir=$(eval echo "$model_dir")
        
        if [[ -d "$expanded_dir" ]]; then
            local model_count
            model_count=$(find "$expanded_dir" -type f -name "*.bin" -o -name "*.safetensors" -o -name "*.gguf" | wc -l)
            if [[ $model_count -gt 0 ]]; then
                success "Model directory $model_dir: $model_count model files found"
            else
                info "Model directory $model_dir: empty (no models downloaded yet)"
            fi
        else
            info "Model directory $model_dir: not found"
        fi
    done
}

check_jupyter_setup() {
    info "Checking Jupyter setup..."
    
    if conda run -n ai-dev jupyter --version >/dev/null 2>&1; then
        success "Jupyter: installed"
        
        # Check JupyterLab
        if conda run -n ai-dev jupyter lab --version >/dev/null 2>&1; then
            success "JupyterLab: installed"
        else
            warning "JupyterLab: not installed"
        fi
        
        # Check Jupyter config
        if [[ -d ~/ai-workspace/.jupyter ]]; then
            success "Jupyter config directory: exists"
        else
            warning "Jupyter config directory: missing"
        fi
        
    else
        error "Jupyter: not installed or not accessible"
    fi
}

run_performance_tests() {
    info "Running performance tests..."
    
    if ! command -v conda >/dev/null 2>&1; then
        warning "Cannot run performance tests - conda not available"
        return 1
    fi
    
    # CPU performance test
    info "Testing CPU performance..."
    local cpu_test_result
    cpu_test_result=$(conda run -n ai-dev python -c "
import time
import numpy as np

start = time.time()
a = np.random.rand(1000, 1000)
b = np.random.rand(1000, 1000)
c = np.dot(a, b)
end = time.time()
print(f'{end-start:.3f}')
" 2>/dev/null || echo "failed")
    
    if [[ "$cpu_test_result" != "failed" ]]; then
        if (( $(echo "$cpu_test_result < 1.0" | bc -l) )); then
            success "CPU performance: excellent (${cpu_test_result}s)"
        elif (( $(echo "$cpu_test_result < 3.0" | bc -l) )); then
            success "CPU performance: good (${cpu_test_result}s)"
        else
            warning "CPU performance: slow (${cpu_test_result}s)"
        fi
    else
        warning "CPU performance test failed"
    fi
    
    # GPU performance test (if available)
    if command -v nvidia-smi >/dev/null 2>&1; then
        info "Testing GPU performance..."
        local gpu_test_result
        gpu_test_result=$(conda run -n ai-dev python -c "
import time
import torch

if torch.cuda.is_available():
    device = torch.device('cuda')
    start = time.time()
    a = torch.rand(1000, 1000, device=device)
    b = torch.rand(1000, 1000, device=device)
    c = torch.mm(a, b)
    torch.cuda.synchronize()
    end = time.time()
    print(f'{end-start:.3f}')
else:
    print('no-cuda')
" 2>/dev/null || echo "failed")
        
        if [[ "$gpu_test_result" == "no-cuda" ]]; then
            warning "GPU performance test: CUDA not available"
        elif [[ "$gpu_test_result" != "failed" ]]; then
            if (( $(echo "$gpu_test_result < 0.1" | bc -l) )); then
                success "GPU performance: excellent (${gpu_test_result}s)"
            elif (( $(echo "$gpu_test_result < 0.5" | bc -l) )); then
                success "GPU performance: good (${gpu_test_result}s)"
            else
                warning "GPU performance: slow (${gpu_test_result}s)"
            fi
        else
            warning "GPU performance test failed"
        fi
    fi
}

generate_diagnosis_report() {
    info "Generating diagnosis report..."
    
    {
        echo "Omarchy AI Diagnosis Report"
        echo "=========================="
        echo "Generated: $(date)"
        echo "System: $(uname -a)"
        echo ""
        
        echo "Summary:"
        echo "- Checks passed: $CHECKS_PASSED"
        echo "- Checks failed: $CHECKS_FAILED"
        echo "- Warnings: $CHECKS_WARNING"
        echo ""
        
        if [[ $CHECKS_FAILED -eq 0 ]]; then
            echo "Overall status: âœ… HEALTHY"
        elif [[ $CHECKS_FAILED -le 2 ]]; then
            echo "Overall status: âš ï¸  NEEDS ATTENTION"
        else
            echo "Overall status: âŒ CRITICAL ISSUES"
        fi
        echo ""
        
        echo "Hardware Information:"
        echo "- RAM: $(($(grep MemTotal /proc/meminfo | awk '{print $2}') / 1024 / 1024))GB"
        echo "- Disk: $(($(df / | tail -1 | awk '{print $4}') / 1024 / 1024))GB available"
        echo "- CPU cores: $(nproc)"
        
        if command -v nvidia-smi >/dev/null 2>&1; then
            echo "- GPU: $(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null || echo 'Detection failed')"
        else
            echo "- GPU: None detected"
        fi
        echo ""
        
        echo "Software Status:"
        if command -v conda >/dev/null 2>&1; then
            echo "- Conda: $(conda --version | awk '{print $2}')"
            if conda info --envs 2>/dev/null | grep -q "ai-dev"; then
                echo "- AI environment: Available"
            else
                echo "- AI environment: Missing"
            fi
        else
            echo "- Conda: Not available"
        fi
        
        if [[ -d ~/ai-workspace ]]; then
            echo "- AI workspace: Available ($(du -sh ~/ai-workspace 2>/dev/null | awk '{print $1}'))"
        else
            echo "- AI workspace: Missing"
        fi
        echo ""
        
        echo "Recommendations:"
        if [[ $CHECKS_FAILED -gt 0 ]]; then
            echo "- Run 'omarchy-ai-repair' to attempt automatic fixes"
            echo "- Check the installation log for detailed error information"
            echo "- Consider re-running the installation if issues persist"
        else
            echo "- System appears to be working correctly"
            echo "- Run periodic diagnostics with 'omarchy-ai-doctor'"
        fi
        echo ""
        
        echo "For detailed logs, check: $LOG_FILE"
        
    } > "$REPORT_FILE"
    
    success "Diagnosis report generated: $REPORT_FILE"
}

# Main diagnostic function
run_diagnostics() {
    info "Starting Omarchy AI diagnostics..."
    info "Log file: $LOG_FILE"
    
    # Initialize log
    echo "Omarchy AI Diagnostics - $(date)" > "$LOG_FILE"
    
    # Reset counters
    CHECKS_PASSED=0
    CHECKS_FAILED=0
    CHECKS_WARNING=0
    
    # Run diagnostic checks
    local diagnostic_checks=(
        "check_omarchy_ai_version"
        "check_system_requirements"
        "check_gpu_status"
        "check_conda_environment"
        "check_python_packages"
        "check_ai_workspace"
        "check_shell_aliases"
        "check_system_services"
        "check_network_connectivity"
        "check_model_files"
        "check_jupyter_setup"
    )
    
    for check in "${diagnostic_checks[@]}"; do
        echo ""
        $check
    done
    
    # Run performance tests if requested
    if [[ "${1:-}" == "--performance" ]]; then
        echo ""
        run_performance_tests
    fi
    
    # Generate report
    echo ""
    generate_diagnosis_report
    
    # Show summary
    echo ""
    echo "================================"
    info "Diagnostics completed"
    echo "Summary: $CHECKS_PASSED passed, $CHECKS_FAILED failed, $CHECKS_WARNING warnings"
    
    if [[ $CHECKS_FAILED -eq 0 ]]; then
        success "System appears to be healthy!"
    elif [[ $CHECKS_FAILED -le 2 ]]; then
        warning "System has some issues but should be functional"
        echo "Run 'omarchy-ai-repair' to attempt fixes"
    else
        error "System has critical issues"
        echo "Consider re-installation or manual troubleshooting"
    fi
    
    echo ""
    echo "ðŸ“„ Full report: $REPORT_FILE"
    echo "ðŸ“‹ Detailed log: $LOG_FILE"
    
    return $CHECKS_FAILED
}

# Show help
show_help() {
    cat << EOF
Omarchy AI Doctor - Diagnostic and Health Check Tool

USAGE:
    $0 [OPTIONS]

OPTIONS:
    --help         Show this help message
    --performance  Run performance benchmarks
    --report-only  Generate report without diagnostics
    --quiet        Suppress verbose output

EXAMPLES:
    $0                    # Run full diagnostics
    $0 --performance      # Include performance tests
    $0 --report-only      # Generate report only

FILES:
    ~/.omarchy-ai-doctor.log       # Diagnostic log
    ~/.omarchy-ai-diagnosis.txt    # Diagnosis report

This tool checks your Omarchy AI installation and identifies potential issues.
Run this periodically to ensure your system is healthy.

EOF
}

# Parse command line arguments
case "${1:-}" in
    --help|-h)
        show_help
        exit 0
        ;;
    --performance)
        run_diagnostics --performance
        ;;
    --report-only)
        generate_diagnosis_report
        ;;
    --quiet)
        run_diagnostics >/dev/null 2>&1
        echo "Diagnostics completed quietly. Check $REPORT_FILE for results."
        ;;
    *)
        run_diagnostics "$@"
        ;;
esac